{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "    DeepSeek-R1 with Amazon Bedrock \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Sufficient local storage space (at least 17GB for 8B and 135GB for 70B models)\n",
    "- (Optional) An Amazon S3 bucket prepared to store the custom model\n",
    "- (Optional) An AWS IAM Role with permissions for Bedrock to read from S3\n",
    "\n",
    "## Step 1: Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!pip install boto3 huggingface_hub transformers[torch] -U",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure parameters\n",
    "Note: When using the defaults, unique random strings will be appended to resource names to prevent naming conflicts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "aws_session_config = Config(retries={'total_max_attempts': 10, 'mode': 'standard'})\n",
    "\n",
    "# Default configuration values\n",
    "default_region = 'us-east-1'\n",
    "default_repository_id = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "default_s3_root_folder = '/'\n",
    "default_s3_bucket_base_name = 'bedrock-imported-models'\n",
    "default_import_role_base_name = 'AmazonBedrockModelImportRole'\n",
    "default_import_policy_name = 'AmazonBedrockModelImportPolicy'\n",
    "\n",
    "# Collect required parameters from user\n",
    "# Allow user to specify custom Hugging Face model repository\n",
    "repository_id = input(f\"Enter Hugging Face repository ID ['{default_repository_id}']: \") or default_repository_id\n",
    "\n",
    "# Allow user to specify AWS region for deployment\n",
    "aws_region = input(f\"Enter the AWS region: ['us-east-1']: \") or default_region\n",
    "\n",
    "# Get IAM role name for model import permissions\n",
    "# If left empty, a new role will be created automatically\n",
    "import_role_name = input(\"Enter the IAM role name for the model import [Leave empty to create a new role]\") or None\n",
    "\n",
    "# Get S3 storage configuration\n",
    "# If left is empty, a new bucket will be created\n",
    "s3_bucket_name = input('Enter the S3 bucket name [Leave empty to create a new bucket]') or None\n",
    "s3_root_folder = input(f\"Enter the S3 root prefix ['{default_s3_root_folder}']\") or default_s3_root_folder\n",
    "\n",
    "# Display final configuration settings\n",
    "print('Configuration:')\n",
    "print(f\"- HF Repository ID: {repository_id}\")\n",
    "print(f\"- Import role ARN: {import_role_name or 'Create a new IAM role'}\")\n",
    "print(f\"- S3 bucket: {s3_bucket_name or 'Create a new S3 bucket'}\")\n",
    "print(f\"- S3 root folder: {s3_root_folder}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create new IAM Role and S3 Bucket if required"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import boto3\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a random resource name postfix\n",
    "postfix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "\n",
    "def get_aws_account_id():\n",
    "    sts_client = boto3.client('sts')\n",
    "    return sts_client.get_caller_identity()['Account']\n",
    "\n",
    "def get_or_create_role(role_name):\n",
    "    iam_client = boto3.client('iam')\n",
    "    \n",
    "    if not role_name:\n",
    "        print('Creating new IAM role...')\n",
    "        \n",
    "        account_id = get_aws_account_id()\n",
    "        role_name = f\"{default_import_role_base_name}-{postfix}\"\n",
    "       \n",
    "        trust_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": { \"Service\": \"bedrock.amazonaws.com\" },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": { \"aws:SourceAccount\": account_id },\n",
    "                    \"ArnEquals\": {\n",
    "                        \"aws:SourceArn\": f\"arn:aws:bedrock:{aws_region}:{account_id}:model-import-job/*\"\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        inline_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:ListBucket\"\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        s3_bucket_arn,\n",
    "                        f\"{s3_bucket_arn}/*\"\n",
    "                    ],\n",
    "                    \"Condition\": {\n",
    "                        \"StringEquals\": {\n",
    "                            \"aws:ResourceAccount\": account_id\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            role = iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "            )\n",
    "\n",
    "            iam_client.put_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyName=f\"{default_import_policy_name}\",\n",
    "                PolicyDocument=json.dumps(inline_policy)\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully created IAM role: {role_name}\")\n",
    "        except ClientError as e:\n",
    "            print(f\"Error creating IAM role: {e}\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(f\"Checking IAM role: {role_name}\")\n",
    "        try:\n",
    "            role = iam_client.get_role(RoleName=role_name)\n",
    "            print('Found existing role.')\n",
    "        except ClientError as e:\n",
    "            print(f\"Error retrieving S3 bucket: {e}\")\n",
    "            exit(1)\n",
    "            \n",
    "    return role[\"Role\"]\n",
    "\n",
    "def get_or_create_bucket(bucket_name):\n",
    "    s3_client = boto3.client('s3', region_name=aws_region)\n",
    "    \n",
    "    if not bucket_name:\n",
    "        print(f\"Creating new S3 bucket...\")\n",
    "        \n",
    "        bucket_name = f\"{default_s3_bucket_base_name}-{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "            \n",
    "            # Wait until bucket exists\n",
    "            waiter = s3_client.get_waiter('bucket_exists')\n",
    "            waiter.wait(\n",
    "                Bucket=bucket_name,\n",
    "                WaiterConfig={\n",
    "                    'Delay': 5,\n",
    "                    'MaxAttempts': 20\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully created S3 bucket: {bucket_name}\")\n",
    "            return bucket_name\n",
    "            \n",
    "        except ClientError as e:\n",
    "            print(f\"Error creating S3 bucket: {e}\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(f\"Checking S3 bucket: {bucket_name}\")\n",
    "        try:\n",
    "            bucket = s3_client.head_bucket(Bucket=bucket_name)\n",
    "            print('Found existing bucket')\n",
    "        except ClientError as e:\n",
    "            print(f\"Error retrieving IAM role: {e}\")\n",
    "            exit(1)\n",
    "            \n",
    "s3_bucket_arn = f\"arn:aws:s3:::{s3_bucket_name}\"\n",
    "import_role = get_or_create_role(import_role_name)\n",
    "import_role_arn = import_role[\"Arn\"]\n",
    "\n",
    "s3_bucket = get_or_create_bucket(s3_bucket_name)\n",
    "s3_bucket_arn = f\"arn:aws:s3:::{s3_bucket_name}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Download and deploy the model\n",
    "## Step 1: Download the weights from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = snapshot_download(repository_id)\n",
    "print(f\"Model downloaded to: {local_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload the weights to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_folder = repository_id if s3_root_folder == '/' else f'{s3_root_folder}/{repository_id}'\n",
    "s3_folder_uri = f\"s3://{s3_bucket_name}/{s3_folder}\"\n",
    "\n",
    "def file_exists_in_s3(bucket_name, s3_key):\n",
    "    return s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_key)['KeyCount'] > 0\n",
    "\n",
    "def upload_to_s3():\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(file_path, local_dir)\n",
    "            s3_key = f\"{s3_folder}/{relative_path}\"\n",
    "            \n",
    "            if file_exists_in_s3(s3_bucket_name, s3_key):\n",
    "                print(f\"Skipping existing file: s3://{s3_bucket_name}/{s3_key}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Uploading: {file_path} to s3://{s3_bucket_name}/{s3_key}\")\n",
    "            s3.upload_file(file_path, s3_bucket_name, s3_key)\n",
    "\n",
    "print('Uploading model files to S3...')\n",
    "\n",
    "upload_to_s3()\n",
    "\n",
    "print(f\"Successfully uploaded model files to {s3_folder_uri}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploy the model to Amazon Bedrock\n",
    "### 3.1: Start a model import job "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import datetime\n",
    "\n",
    "bedrock = boto3.client('bedrock', region_name=aws_region)\n",
    "model_name = repository_id.split('/')[-1].replace('.', '-').replace('_', '-')\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "job_name = f'{model_name}-{timestamp}'\n",
    "\n",
    "print(f\"Starting model import job: {job_name}\")\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=model_name,\n",
    "    roleArn=import_role_arn,\n",
    "    modelDataSource={'s3DataSource': {'s3Uri': s3_folder_uri}}\n",
    ")\n",
    "\n",
    "print(f\"Model import job started\")\n",
    "\n",
    "job_arn = response['jobArn']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Monitor the import job status"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "print(f\"Checking status of import job: {job_name}\")\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_arn)\n",
    "    status = response['status']\n",
    "    if status == 'Failed':\n",
    "        print('Model import failed!')\n",
    "        \n",
    "        failure_message = response['failureMessage']\n",
    "        print(f\"Reason: {failure_message}\")\n",
    "        break\n",
    "    elif status == 'Completed':\n",
    "        print('Model import complete.')\n",
    "        \n",
    "        model_id = response['importedModelArn']\n",
    "        print(f\"Imported model ID: {model_id}\")\n",
    "        break\n",
    "    else:\n",
    "        print('Importing...')\n",
    "\n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "    \n",
    "model_arn = response['importedModelArn']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: If necessary, wait some more time to make sure the model has been initialized"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Wait for 5 minutes for model initialization \n",
    "print('Waiting 5 minutes for model initialization...')\n",
    "\n",
    "for i in range(5, 0, -1):\n",
    "    print(f'{i} minute{\"s\" if i > 1 else \"\"}...')\n",
    "    time.sleep(60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "\n",
    "## Step 1: Prepare the prompt "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the message you want to send to the model\n",
    "user_message = \"Explain the concept of 'rubber duck debugging' in a single paragraph.\"\n",
    "\n",
    "# Apply the `user` role and embed it in a list of messages\n",
    "messages = [\n",
    "    {'role': 'user','content': user_message}\n",
    "]\n",
    "\n",
    "# For optimal results, apply the specific model's chat template\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repository_id)\n",
    "\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(f\"Formatted prompt: \\n{formatted_prompt}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Invoke the model\n",
    " \n",
    "### Option 2.1: Using the `InvokeModelWithResponseStream` API\n",
    "This allows you to retrieve and process the response in real-time as chunks.\n",
    "\n",
    "**Pros:** Lower latency to first displayed content; provides visual feedback during generation; reduced memory overhead for large responses\n",
    "\n",
    "**Cons:** More complex implementation; requires handling partial/incomplete responses; need to manage stream state and error handling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a function to invoke the model and process the response stream in real-time\n",
    "def invoke_model_with_response_stream(prompt, region_name=aws_region, max_tokens=4096):\n",
    "    global model_arn\n",
    "    global aws_session_config\n",
    "        \n",
    "    session = boto3.session.Session()\n",
    "    br_runtime = session.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=region_name,\n",
    "        config=aws_session_config\n",
    "    )\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    streaming_response = br_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_arn,\n",
    "        body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    final_response = \"\"\n",
    "    for event in streaming_response[\"body\"]:\n",
    "        chunk = json.loads(event['chunk']['bytes'])\n",
    "        if \"generation\" in chunk:\n",
    "            text = chunk[\"generation\"]\n",
    "            final_response += text \n",
    "            print(text, end=\"\", flush=True)\n",
    "            \n",
    "    return final_response\n",
    "\n",
    "# Invoke the model with the formatted prompt and observe its output in real-time\n",
    "response = invoke_model_with_response_stream(formatted_prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2.2: Using the `InvokeModel` API\n",
    "This provides a single, complete response after model processing is finished.\n",
    "\n",
    "**Pros:** Simpler implementation; response arrives fully formatted; easier to use for downstream processing\n",
    "\n",
    "**Cons:** Longer perceived wait time; no visual feedback during processing; entire response must be held in memory at once"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a function to invoke the model and return the complete response\n",
    "def invoke_model(model, message, region_name=aws_region, max_tokens=4096):\n",
    "    global model_arn\n",
    "    global aws_session_config\n",
    "        \n",
    "    session = boto3.session.Session()\n",
    "    br_runtime = session.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=region_name,\n",
    "        config=aws_session_config\n",
    "    )\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "        \n",
    "    try:\n",
    "        complete_response = br_runtime.invoke_model(\n",
    "            modelId=model, \n",
    "            body=json.dumps(payload) \n",
    "        )\n",
    "        result = json.loads(complete_response[\"body\"].read().decode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(e.__repr__())\n",
    "\n",
    "    return result[\"generation\"]\n",
    "\n",
    "response = invoke_model(model=model_arn, message=user_message)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3: Display the formatted final response"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython import display\n",
    "\n",
    "def try_convert_to_markdown(raw_response):\n",
    "    # When reasoning section (<think>...</think>) is present, convert the response into markdown\n",
    "    if '<think>' in raw_response and '</think>' in raw_response:\n",
    "        # Extract the reasoning content between <think> tags\n",
    "        think_start = raw_response.find('<think>') + len('<think>')\n",
    "        think_end = raw_response.find('</think>')\n",
    "        reasoning_section = raw_response[think_start:think_end]\n",
    "\n",
    "        # Get the actual response that follows the reasoning section\n",
    "        response_section = raw_response[raw_response.find('</think>') + len('</think>'):]\n",
    "\n",
    "        # Format the output with markdown headers to visually separate the reasoning from the final response\n",
    "        return (f\"\\n**Reasoning:**\\n\" \\\n",
    "                f\"\\n{reasoning_section.strip()}\\n\" \\\n",
    "                f\"\\n**Response:**\\n\" \\\n",
    "                f\"\\n{response_section.strip()}\\n\")\n",
    "\n",
    "    else:\n",
    "        # When no reasoning section is present, return the raw generation without any special formatting\n",
    "        return raw_response\n",
    "\n",
    "# Convert the model output to markdown format and display it\n",
    "converted_response_text = try_convert_to_markdown(response)\n",
    "display.Markdown(converted_response_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
